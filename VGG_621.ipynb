{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_621.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/stpraha/vgg_fer2013/blob/master/VGG_621.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vOI9qb_RY5u1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Jun 16 10:00:07 2018\n",
        "\n",
        "@author: Administrator\n",
        "\"\"\"\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "\n",
        "f_train = open(\"drive//fer2013//traincpy.csv\", encoding = 'UTF-8')\n",
        "df_train = pd.read_csv(f_train)\n",
        "f_test_pub = open(\"drive//fer2013//valcpy.csv\", encoding = 'UTF-8')\n",
        "df_test_pub = pd.read_csv(f_test_pub)\n",
        "f_test_pri = open(\"drive//fer2013//testcpy.csv\", encoding = 'UTF-8')\n",
        "df_test_pri = pd.read_csv(f_test_pri)\n",
        "\n",
        "train_featuresets = df_train.iloc[1: , 1: ]\n",
        "train_emotionsets = df_train.iloc[1: , 0:1]\n",
        "test_pub_featuresets = df_test_pub.iloc[0: , 1: ]\n",
        "test_pub_emotionsets = df_test_pub.iloc[0: , 0:1]\n",
        "test_pri_featuresets = df_test_pri.iloc[0: , 1: ]\n",
        "test_pri_emotionsets = df_test_pri.iloc[0: , 0:1]\n",
        "\n",
        "#train_feature = tf.constant(train_featuresets)\n",
        "#train_emotion = tf.constant(train_emotionsets)\n",
        "\n",
        "#train_feature = tf.reshape(train_feature, [-1, 48, 48, 1])\n",
        "#train_emotion = tf.reshape(train_emotion, [-1, 1])\n",
        "\n",
        "train_feature = np.reshape(np.array(train_featuresets, dtype = 'float32'), (-1, 48, 48, 1))\n",
        "train_emotion = np.reshape(np.array(train_emotionsets, dtype = 'float32'), (-1))\n",
        "\n",
        "test_pub_feature = np.reshape(np.array(test_pub_featuresets, dtype = 'float32'), (-1, 48, 48, 1))\n",
        "test_pub_emotion = np.reshape(np.array(test_pub_emotionsets, dtype = 'float32'), (-1))\n",
        "\n",
        "test_pri_feature = np.reshape(np.array(test_pri_featuresets, dtype = 'float32'), (-1, 48, 48, 1))\n",
        "test_pri_emotion = np.reshape(np.array(test_pri_emotionsets, dtype = 'float32'), (-1))\n",
        "\n",
        "\n",
        "#print(train_feature)\n",
        "#print(train_feature[0:32])\n",
        "#print(train_feature.get_shape()[0].value)\n",
        "\n",
        "batch_size = 32\n",
        "num_batches = 100\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "X = tf.placeholder(tf.float32, [32, 48, 48, 1])\n",
        "Y = tf.placeholder(tf.int32)\n",
        "    \n",
        "# 用来创建卷积层并把本层的参数存入参数列表\n",
        "# input_op:输入的tensor name:该层的名称 kh:卷积层的高 kw:卷积层的宽 n_out:输出通道数，dh:步长的高 dw:步长的宽，p是参数列表\n",
        "def conv_op(input_op, name, kh, kw, n_out, dh, dw, p):\n",
        "    #获取input_op的通道数\n",
        "    n_in = input_op.get_shape()[-1].value\n",
        "    with tf.name_scope(name) as scope:\n",
        "        #卷积核参数\n",
        "        kernel = tf.get_variable(scope + \"w\", shape = [kh, kw, n_in, n_out], dtype = tf.float32, initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
        "        #对input_op进行卷积处理，卷及和为kernel，步长\n",
        "        #第一个参数需要做卷积的输入图像，是一个Tensor，[batch, in_height, in_width, in_channels]是一个4维的Tensor，float32和float64之一\n",
        "        #第二个参数相当于CNN中的卷积核，是一个Tensor，[filter_height, filter_width, in_channels, out_channels]类型与参数input相同，第三维in_channels，是input的第四维\n",
        "        #第三个参数卷积时在图像每一维的步长，这是一个一维的向量，长度4\n",
        "        #第四个参数padding：string类型的量，只能是\"SAME\",\"VALID\"其中之一，SAME可以停留在图像边缘\n",
        "        #结果返回一个Tensor，这个输出，就是我们常说的feature map，shape仍然是[batch, height, width, channels]\n",
        "        conv = tf.nn.conv2d(input_op, kernel, (1, dh, dw, 1), padding = \"SAME\")\n",
        "        #创建一个张量，用0.0来填充\n",
        "        bias_init_val = tf.constant(0.0, shape = [n_out], dtype = tf.float32)\n",
        "        #转成可训练的参数，可以对他用Optimizer\n",
        "        biases = tf.Variable(bias_init_val, trainable = True, name = 'b')\n",
        "        #将偏差项bias加到conv上面，这里是bias必须是一维的\n",
        "        z = tf.nn.bias_add(conv, biases)\n",
        "        #卷积层的输出\n",
        "        activation = tf.nn.relu(z, name = scope)\n",
        "        #将kernel和biases加到参数列表\n",
        "        p += [kernel, biases]\n",
        "        return activation\n",
        "\n",
        "#定义全连接层\n",
        "def fc_op(input_op, name, n_out, p):\n",
        "    #获取通道数\n",
        "    n_in = input_op.get_shape()[-1].value\n",
        "    \n",
        "    with tf.name_scope(name) as scope:\n",
        "        #创建全连接层的参数，只有两个维度，也用xavier_initializer来初始化\n",
        "        kernel = tf.get_variable(scope+\"w\", shape = [n_in, n_out], dtype = tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
        "        #初始化biases，这里用0.1来填充了\n",
        "        biases = tf.Variable(tf.constant(0.1, shape = [n_out], dtype = tf.float32), name = 'b')\n",
        "        activation = tf.nn.relu_layer(input_op, kernel, biases, name = scope)\n",
        "        p += [kernel, biases]\n",
        "        return activation\n",
        "\n",
        "#定义最大池化层的创建函数\n",
        "#maxpool即领域内取最大\n",
        "def mpool_op(input_op, name, kh, kw, dh, dw):\n",
        "    #这里tf.nn.max_pool(value, ksize, strides, padding, name=None)\n",
        "    #value输入通常是feature map\n",
        "    #池化窗口的大小，不再batch和channel上池化，所以两个为1\n",
        "    #窗口在每个维度上的滑动步长\n",
        "    #和卷积类似\n",
        "    #返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式\n",
        "    return tf.nn.max_pool(input_op, ksize = [1, kh, kw, 1], strides = [1, dh, dw, 1], padding = 'SAME', name = name)\n",
        "\n",
        "\n",
        "#创建VGGNET-16的网络结构\n",
        "def inference_op(input_op, keep_prob):\n",
        "    p = []\n",
        "    conv1_1 = conv_op(input_op, name = \"conv1_1\", kh = 3, kw = 3, n_out = 64, dh = 1, dw = 1, p = p)\n",
        "    conv1_2 = conv_op(conv1_1, name = \"conv1_2\", kh = 3, kw = 3, n_out = 64, dh = 1, dw = 1, p = p)\n",
        "    #这里每次都会输出结果的边长减半，但是通道数加倍了\n",
        "    pool1 = mpool_op(conv1_2, name = \"pool1\", kh = 2, kw = 2, dw = 2, dh = 2)\n",
        "    \n",
        "    conv2_1 = conv_op(pool1, name = \"conv2_1\", kh = 3, kw = 3, n_out = 128, dh = 1, dw = 1, p = p)\n",
        "    conv2_2 = conv_op(conv2_1, name = \"conv2_2\", kh = 3, kw = 3, n_out = 128, dh = 1, dw = 1, p = p)\n",
        "    pool2 = mpool_op(conv2_2, name = \"pool1\", kh = 2, kw = 2, dw = 2, dh = 2)\n",
        "    \n",
        "    conv3_1 = conv_op(pool2, name = \"conv3_1\", kh = 3, kw = 3, n_out = 256, dh = 1, dw = 1, p = p)\n",
        "    conv3_2 = conv_op(conv3_1, name = \"conv3_2\", kh = 3, kw = 3, n_out = 256, dh = 1, dw = 1, p = p)\n",
        "    conv3_3 = conv_op(conv3_2, name = \"conv3_3\", kh = 3, kw = 3, n_out = 256, dh = 1, dw = 1, p = p)\n",
        "    pool3 = mpool_op(conv3_3, name = \"pool3\", kh = 2, kw = 2, dh = 2, dw = 2)\n",
        "    \n",
        "    conv4_1 = conv_op(pool3, name = \"conv4_1\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    conv4_2 = conv_op(conv4_1, name = \"conv4_2\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    conv4_3 = conv_op(conv4_2, name = \"conv4_3\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    pool4 = mpool_op(conv4_3, name = \"pool4\", kh = 2, kw = 2, dh = 2, dw = 2)\n",
        "    \n",
        "    conv5_1 = conv_op(pool4, name = \"conv5_1\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    conv5_2 = conv_op(conv5_1, name = \"conv5_2\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    conv5_3 = conv_op(conv5_2, name = \"conv5_3\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    pool5 = mpool_op(conv5_3, name = \"pool5\", kh = 2, kw = 2, dh = 2, dw = 2)\n",
        "    \n",
        "    shp = pool5.get_shape()\n",
        "    #将每个样本化为长度为（长*宽*通道）的一维向量\n",
        "    flattened_shape = shp[1].value * shp[2].value * shp[3].value\n",
        "    resh1 = tf.reshape(pool5, [-1, flattened_shape], name = \"resh1\")\n",
        "    \n",
        "    #链接到一个隐含节点为4096的全连接层\n",
        "    fc6 = fc_op(resh1, name = \"fc6\", n_out = 4096, p = p)\n",
        "    #dropout防止或减轻过拟合而使用的函数，它一般用在全连接层。\n",
        "    #Dropout就是在不同的训练过程中随机扔掉一部分神经元。\n",
        "    #训练时的保留率为0.5，预测时为1.0\n",
        "    fc6_drop = tf.nn.dropout(fc6, keep_prob, name = \"fc6_drop\")\n",
        "    \n",
        "    fc7 = fc_op(fc6_drop, name = \"fc7\", n_out = 4096, p = p)\n",
        "    fc7_drop = tf.nn.dropout(fc7, keep_prob, name = \"fc7_drop\")\n",
        "    \n",
        "    fc8 = fc_op(fc7_drop, name = \"fc8\", n_out = 7, p = p)\n",
        "    #得到分类输出概率\n",
        "    softmax = tf.nn.softmax(fc8)\n",
        "    #得到概率最大的类别\n",
        "    predictions = tf.argmax(softmax, 1)\n",
        "    #print('in inference op : softmax', softmax)\n",
        "    #print('in inference op : prediction',predictions)\n",
        "    return predictions, softmax\n",
        "\n",
        "\n",
        "#这里通道变多可以增加表达能力，每个通道都是由一个卷积核算出来的，有的特征对不同的卷积核敏感，多通道可以把他们都保留下来\n",
        "def train_vgg():\n",
        "    predictions, softmax = inference_op(X, keep_prob)\n",
        "    \n",
        "    #print('in train vgg softmax', softmax)\n",
        "    \n",
        "    \n",
        "    #\n",
        "    #这里肯定有问题\n",
        "    #\n",
        "    cross_entropy = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits = softmax))\n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    \n",
        "    train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
        "    \n",
        "    #初始化全局参数\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        print('going to strat train')\n",
        "        for i in range(100):\n",
        "            start = 0\n",
        "            end = start + batch_size\n",
        "            step = 0\n",
        "            while(end < len(train_feature)):          \n",
        "                loss_ = sess.run([predictions, loss], feed_dict = {X:train_feature[start:end], keep_prob:0.5, Y:train_emotion[start:end]})\n",
        "                start += batch_size\n",
        "                end += batch_size\n",
        "                if step%100 == 0:\n",
        "                    #print(tf.argmax(softmax, 1).eval())\n",
        "                    print('save model   round: ', i,'step: ' , step, 'the loss: ', loss_)\n",
        "                step += 1\n",
        "            if i%5 == 0:\n",
        "                use_vgg()\n",
        "\n",
        "def use_vgg():\n",
        "    pred, softmax = inference_op(X, keep_prob)\n",
        "    prediction = []\n",
        "    with tf.Session() as sess:\n",
        "        print('goint to start public prediction')\n",
        "        print('the length of test_pub_emotion :' ,len(test_pub_emotion))\n",
        "        \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        start = 0\n",
        "        end = start + batch_size\n",
        "        k = 0\n",
        "        while(end < len(test_pub_feature)):\n",
        "            predict = sess.run(pred, feed_dict = {X:test_pub_feature[start:end], keep_prob:1})\n",
        "            prediction.append(predict.tolist())\n",
        "            accurate = test_pub_emotion[start:end]\n",
        "            \n",
        "            if  end%512 == 0:\n",
        "                print(predict)\n",
        "                print(accurate)\n",
        "                \n",
        "            for i in range(len(predict)):\n",
        "                if predict[i] == accurate[i]:\n",
        "                    #print(predict[i], accurate[i])\n",
        "                    k += 1                   \n",
        "            start += batch_size\n",
        "            end += batch_size\n",
        "            \n",
        "        accurate_rate = k / len(test_pub_emotion)\n",
        "        print('end public prediction')\n",
        "        print('the public accurate is : ', accurate_rate)\n",
        "          \n",
        "    with tf.Session() as sess:\n",
        "        print('goint to start private prediction')\n",
        "        print('the length of test_pri_emotion :' ,len(test_pri_emotion))\n",
        "        \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        start = 0\n",
        "        end = start + batch_size\n",
        "        k = 0\n",
        "        while(end < len(test_pri_feature)):\n",
        "            predict = sess.run(pred, feed_dict = {X:test_pri_feature[start:end], keep_prob:1})\n",
        "            prediction.append(predict.tolist())\n",
        "            accurate = test_pri_emotion[start:end]\n",
        "            \n",
        "            #if  end == 64:\n",
        "                #print(predict)\n",
        "                #print(accurate)\n",
        "                \n",
        "            for i in range(len(predict)):\n",
        "                if predict[i] == accurate[i]:\n",
        "                    #print(predict[i], accurate[i])\n",
        "                    k += 1                   \n",
        "            start += batch_size\n",
        "            end += batch_size\n",
        "            \n",
        "        accurate_rate = k / len(test_pri_emotion)\n",
        "        print('end private prediction')\n",
        "        print('the private accurate is : ', accurate_rate)\n",
        "        \n",
        "        \n",
        "train_vgg()\n",
        "#use_vgg()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fj18u_K-o13m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "b90564e9-7f36-4a0e-b66d-2e0ccff6c536"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gcgltATMo2pQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0aafb109-1d95-4263-dfe9-07e450cc923e"
      },
      "cell_type": "code",
      "source": [
        "# 指定Google Drive云端硬盘的根目录，名为drive\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\r\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}