{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_621.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/stpraha/vgg_fer2013/blob/master/VGG_621.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "vOI9qb_RY5u1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1111
        },
        "outputId": "a8200431-b023-40e5-8928-a50df75fd639"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Jun 16 10:00:07 2018\n",
        "\n",
        "@author: Administrator\n",
        "\"\"\"\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "\n",
        "f_train = open(\"drive//fer2013//traincpy.csv\", encoding = 'UTF-8')\n",
        "df_train = pd.read_csv(f_train)\n",
        "#f_train_resize = open(\"drive//fer2013//trainresize.csv\", encoding = 'UTF-8')\n",
        "#df_train_resize = pd.read_csv(f_train_resize)\n",
        "f_test_pub = open(\"drive//fer2013//valcpy.csv\", encoding = 'UTF-8')\n",
        "df_test_pub = pd.read_csv(f_test_pub)\n",
        "f_test_pri = open(\"drive//fer2013//testcpy.csv\", encoding = 'UTF-8')\n",
        "df_test_pri = pd.read_csv(f_test_pri)\n",
        "print('read csv file finished')\n",
        "\n",
        "\n",
        "train_featuresets = df_train.iloc[1: , 1: ]\n",
        "train_emotionsets = df_train.iloc[1: , 0:1]\n",
        "test_pub_featuresets = df_test_pub.iloc[0: , 1: ]\n",
        "test_pub_emotionsets = df_test_pub.iloc[0: , 0:1]\n",
        "test_pri_featuresets = df_test_pri.iloc[0: , 1: ]\n",
        "test_pri_emotionsets = df_test_pri.iloc[0: , 0:1]\n",
        "print('dataset load finished')\n",
        "\n",
        "#train_featuresets_resize = df_train_resize.iloc[1: , 0: ]\n",
        "#train_feature_resize = tf.constant(train_featuresets_resize)\n",
        "#train_feature_resize = tf.reshape(train_feature_resize, [-1, 224, 224, 1])\n",
        "#train_emotion = np.reshape(np.array(train_emotionsets, dtype = 'float32'), (-1))\n",
        "#print(train_feature_resize.shape)\n",
        "\n",
        "#train_feature = tf.constant(train_featuresets)\n",
        "#train_emotion = tf.constant(train_emotionsets)\n",
        "\n",
        "#train_feature = tf.reshape(train_feature, [-1, 48, 48, 1])\n",
        "#train_emotion = tf.reshape(train_emotion, [-1, 1])\n",
        "\n",
        "#双线性插值，讲48*48的图片变成224*224\n",
        "\n",
        "def resize(src, new_size):\n",
        "    dst_w = 224\n",
        "    dst_h = 224 # 目标图像宽高\n",
        "    src_h = 48\n",
        "    src_w = 48 # 源图像宽高\n",
        "    if src_h == dst_h and src_w == dst_w:\n",
        "        return src.copy()\n",
        "    scale_x = float(src_w) / dst_w # x缩放比例\n",
        "    scale_y = float(src_h) / dst_h # y缩放比例\n",
        "\n",
        "    # 遍历目标图像，插值\n",
        "    dst = np.zeros((dst_h, dst_w, 1), dtype=np.uint8)\n",
        "    for n in range(1): # 对channel循环\n",
        "        for dst_y in range(dst_h): # 对height循环\n",
        "            for dst_x in range(dst_w): # 对width循环\n",
        "                # 目标在源上的坐标\n",
        "                src_x = (dst_x + 0.5) * scale_x - 0.5\n",
        "                src_y = (dst_y + 0.5) * scale_y - 0.5\n",
        "                # 计算在源图上四个近邻点的位置\n",
        "                src_x_0 = int(np.floor(src_x))\n",
        "                src_y_0 = int(np.floor(src_y))\n",
        "                src_x_1 = min(src_x_0 + 1, src_w - 1)\n",
        "                src_y_1 = min(src_y_0 + 1, src_h - 1)\n",
        "\n",
        "                #双线性插值\n",
        "                value0 = (src_x_1 - src_x) * src[src_y_0, src_x_0, n] + (src_x - src_x_0) * src[src_y_0, src_x_1, n]\n",
        "                value1 = (src_x_1 - src_x) * src[src_y_1, src_x_0, n] + (src_x - src_x_0) * src[src_y_1, src_x_1, n]\n",
        "                dst[dst_y, dst_x, n] = int((src_y_1 - src_y) * value0 + (src_y - src_y_0) * value1)\n",
        "    return dst\n",
        "\n",
        "\n",
        "print('start resize 48*48 to 224*224')\n",
        "train_feature_resize = []\n",
        "train_feature = np.reshape(np.array(train_featuresets, dtype = 'float32'), (-1, 48, 48, 1))\n",
        "train_emotion = np.reshape(np.array(train_emotionsets, dtype = 'float32'), (-1))\n",
        "print('total ', train_feature.shape[0])\n",
        "for i in range(train_feature.shape[0]):\n",
        "#for i in range(640):\n",
        "    if i%1000 == 0:\n",
        "        print('now resize 48 --> 224 train set',i)\n",
        "    train_feature_resize.append(resize(train_feature[i], 224))\n",
        "train_feature_resize = np.reshape(np.array(train_feature_resize, dtype = 'float32'), (-1, 224, 224,1))\n",
        "print(train_feature_resize.shape)\n",
        "print('train_feature resize finished')\n",
        "\n",
        "test_pub_feature_resize = []\n",
        "test_pub_feature = np.reshape(np.array(test_pub_featuresets, dtype = 'float32'), (-1, 48, 48, 1))\n",
        "test_pub_emotion = np.reshape(np.array(test_pub_emotionsets, dtype = 'float32'), (-1))\n",
        "for i in range(test_pub_feature.shape[0]):\n",
        "##for i in range(320):\n",
        "    if i%200 == 0:\n",
        "        print('now resize 48 --> 224 pub test set',i)\n",
        "    test_pub_feature_resize.append(resize(test_pub_feature[i], 224))\n",
        "test_pub_feature_resize = np.reshape(np.array(test_pub_feature_resize, dtype = 'float32'), (-1, 224, 224,1))\n",
        "print(test_pub_feature_resize.shape)\n",
        "print('test_pub resize finished')\n",
        "\n",
        "test_pri_feature_resize = []\n",
        "test_pri_feature = np.reshape(np.array(test_pri_featuresets, dtype = 'float32'), (-1, 48, 48, 1))\n",
        "test_pri_emotion = np.reshape(np.array(test_pri_emotionsets, dtype = 'float32'), (-1))\n",
        "for i in range(test_pri_feature.shape[0]):\n",
        "#for i in range(320):\n",
        "    if i%200 == 0:\n",
        "        print('now resize 48 --> 224 pri test set',i)\n",
        "    test_pri_feature_resize.append(resize(test_pri_feature[i], 224))\n",
        "test_pri_feature_resize = np.reshape(np.array(test_pri_feature_resize, dtype = 'float32'), (-1, 224, 224,1))\n",
        "print(test_pri_feature_resize.shape)\n",
        "print('test_pri resize finished')\n",
        "#print(train_feature[0:2])\n",
        "\n",
        "batch_size = 32\n",
        "num_batches = 100\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "X = tf.placeholder(tf.float32, [32, 224, 224, 1])\n",
        "Y = tf.placeholder(tf.int32)\n",
        "    \n",
        "# 用来创建卷积层并把本层的参数存入参数列表\n",
        "# input_op:输入的tensor name:该层的名称 kh:卷积层的高 kw:卷积层的宽 n_out:输出通道数，dh:步长的高 dw:步长的宽，p是参数列表\n",
        "def conv_op(input_op, name, kh, kw, n_out, dh, dw, p):\n",
        "    #获取input_op的通道数\n",
        "    n_in = input_op.get_shape()[-1].value\n",
        "    with tf.name_scope(name) as scope:\n",
        "        #卷积核参数\n",
        "        kernel = tf.get_variable(scope + \"w\", shape = [kh, kw, n_in, n_out], dtype = tf.float32, initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
        "        #对input_op进行卷积处理，卷及和为kernel，步长\n",
        "        #第一个参数需要做卷积的输入图像，是一个Tensor，[batch, in_height, in_width, in_channels]是一个4维的Tensor，float32和float64之一\n",
        "        #第二个参数相当于CNN中的卷积核，是一个Tensor，[filter_height, filter_width, in_channels, out_channels]类型与参数input相同，第三维in_channels，是input的第四维\n",
        "        #第三个参数卷积时在图像每一维的步长，这是一个一维的向量，长度4\n",
        "        #第四个参数padding：string类型的量，只能是\"SAME\",\"VALID\"其中之一，SAME可以停留在图像边缘\n",
        "        #结果返回一个Tensor，这个输出，就是我们常说的feature map，shape仍然是[batch, height, width, channels]\n",
        "        conv = tf.nn.conv2d(input_op, kernel, (1, dh, dw, 1), padding = \"SAME\")\n",
        "        #创建一个张量，用0.0来填充\n",
        "        bias_init_val = tf.constant(0.0, shape = [n_out], dtype = tf.float32)\n",
        "        #转成可训练的参数，可以对他用Optimizer\n",
        "        biases = tf.Variable(bias_init_val, trainable = True, name = 'b')\n",
        "        #将偏差项bias加到conv上面，这里是bias必须是一维的\n",
        "        z = tf.nn.bias_add(conv, biases)\n",
        "        #卷积层的输出\n",
        "        activation = tf.nn.relu(z, name = scope)\n",
        "        #将kernel和biases加到参数列表\n",
        "        p += [kernel, biases]\n",
        "        return activation\n",
        "\n",
        "#定义全连接层\n",
        "def fc_op(input_op, name, n_out, p):\n",
        "    #获取通道数\n",
        "    n_in = input_op.get_shape()[-1].value\n",
        "    \n",
        "    with tf.name_scope(name) as scope:\n",
        "        #创建全连接层的参数，只有两个维度，也用xavier_initializer来初始化\n",
        "        kernel = tf.get_variable(scope+\"w\", shape = [n_in, n_out], dtype = tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
        "        #初始化biases，这里用0.1来填充了\n",
        "        biases = tf.Variable(tf.constant(0.1, shape = [n_out], dtype = tf.float32), name = 'b')\n",
        "        activation = tf.nn.relu_layer(input_op, kernel, biases, name = scope)\n",
        "        p += [kernel, biases]\n",
        "        return activation\n",
        "\n",
        "#定义最大池化层的创建函数\n",
        "#maxpool即领域内取最大\n",
        "def mpool_op(input_op, name, kh, kw, dh, dw):\n",
        "    #这里tf.nn.max_pool(value, ksize, strides, padding, name=None)\n",
        "    #value输入通常是feature map\n",
        "    #池化窗口的大小，不再batch和channel上池化，所以两个为1\n",
        "    #窗口在每个维度上的滑动步长\n",
        "    #和卷积类似\n",
        "    #返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式\n",
        "    return tf.nn.max_pool(input_op, ksize = [1, kh, kw, 1], strides = [1, dh, dw, 1], padding = 'SAME', name = name)\n",
        "\n",
        "\n",
        "#创建VGGNET-16的网络结构\n",
        "def inference_op(input_op, keep_prob):\n",
        "    p = []\n",
        "    conv1_1 = conv_op(input_op, name = \"conv1_1\", kh = 3, kw = 3, n_out = 64, dh = 1, dw = 1, p = p)\n",
        "    conv1_2 = conv_op(conv1_1, name = \"conv1_2\", kh = 3, kw = 3, n_out = 64, dh = 1, dw = 1, p = p)\n",
        "    #这里每次都会输出结果的边长减半，但是通道数加倍了\n",
        "    pool1 = mpool_op(conv1_2, name = \"pool1\", kh = 2, kw = 2, dw = 2, dh = 2)\n",
        "    \n",
        "    conv2_1 = conv_op(pool1, name = \"conv2_1\", kh = 3, kw = 3, n_out = 128, dh = 1, dw = 1, p = p)\n",
        "    conv2_2 = conv_op(conv2_1, name = \"conv2_2\", kh = 3, kw = 3, n_out = 128, dh = 1, dw = 1, p = p)\n",
        "    pool2 = mpool_op(conv2_2, name = \"pool1\", kh = 2, kw = 2, dw = 2, dh = 2)\n",
        "    \n",
        "    conv3_1 = conv_op(pool2, name = \"conv3_1\", kh = 3, kw = 3, n_out = 256, dh = 1, dw = 1, p = p)\n",
        "    conv3_2 = conv_op(conv3_1, name = \"conv3_2\", kh = 3, kw = 3, n_out = 256, dh = 1, dw = 1, p = p)\n",
        "    conv3_3 = conv_op(conv3_2, name = \"conv3_3\", kh = 3, kw = 3, n_out = 256, dh = 1, dw = 1, p = p)\n",
        "    pool3 = mpool_op(conv3_3, name = \"pool3\", kh = 2, kw = 2, dh = 2, dw = 2)\n",
        "    \n",
        "    conv4_1 = conv_op(pool3, name = \"conv4_1\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    conv4_2 = conv_op(conv4_1, name = \"conv4_2\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    conv4_3 = conv_op(conv4_2, name = \"conv4_3\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    pool4 = mpool_op(conv4_3, name = \"pool4\", kh = 2, kw = 2, dh = 2, dw = 2)\n",
        "    \n",
        "    conv5_1 = conv_op(pool4, name = \"conv5_1\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    conv5_2 = conv_op(conv5_1, name = \"conv5_2\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    conv5_3 = conv_op(conv5_2, name = \"conv5_3\", kh = 3, kw = 3, n_out = 512, dh = 1, dw = 1, p = p)\n",
        "    pool5 = mpool_op(conv5_3, name = \"pool5\", kh = 2, kw = 2, dh = 2, dw = 2)\n",
        "    \n",
        "    shp = pool5.get_shape()\n",
        "    #将每个样本化为长度为（长*宽*通道）的一维向量\n",
        "    flattened_shape = shp[1].value * shp[2].value * shp[3].value\n",
        "    resh1 = tf.reshape(pool5, [-1, flattened_shape], name = \"resh1\")\n",
        "    \n",
        "    #链接到一个隐含节点为4096的全连接层\n",
        "    fc6 = fc_op(resh1, name = \"fc6\", n_out = 4096, p = p)\n",
        "    #dropout防止或减轻过拟合而使用的函数，它一般用在全连接层。\n",
        "    #Dropout就是在不同的训练过程中随机扔掉一部分神经元。\n",
        "    #训练时的保留率为0.5，预测时为1.0\n",
        "    fc6_drop = tf.nn.dropout(fc6, keep_prob, name = \"fc6_drop\")\n",
        "    #fc6_drop = fc6\n",
        "    fc7 = fc_op(fc6_drop, name = \"fc7\", n_out = 4096, p = p)\n",
        "    fc7_drop = tf.nn.dropout(fc7, keep_prob, name = \"fc7_drop\")\n",
        "    #fc7_drop = fc7\n",
        "    fc8 = fc_op(fc7_drop, name = \"fc8\", n_out = 7, p = p)\n",
        "    #得到分类输出概率\n",
        "    softmax = tf.nn.softmax(fc8)\n",
        "    #得到概率最大的类别\n",
        "    predictions = tf.argmax(softmax, 1)\n",
        "    #print('in inference op : softmax', softmax)\n",
        "    #print('in inference op : prediction',predictions)\n",
        "    return predictions, softmax\n",
        "\n",
        "\n",
        "#这里通道变多可以增加表达能力，每个通道都是由一个卷积核算出来的，有的特征对不同的卷积核敏感，多通道可以把他们都保留下来\n",
        "def train_vgg():\n",
        "    predictions, softmax = inference_op(X, keep_prob)    \n",
        "    #\n",
        "    #这里肯定有问题\n",
        "    #\n",
        "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits = softmax))\n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    \n",
        "    train_op = tf.train.GradientDescentOptimizer(0.006).minimize(loss)\n",
        "    \n",
        "    #初始化全局参数\n",
        "    max_pub = 0\n",
        "    max_pri = 0\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        print('now strat train')\n",
        "        print('the length of train_emotion:', len(train_emotion))\n",
        "        for i in range(100):\n",
        "            start = 0\n",
        "            end = start + batch_size\n",
        "            step = 0\n",
        "            while(end < len(train_emotion)):\n",
        "            #while(end < 640):\n",
        "                #这里可能有问题\n",
        "                train_op.run(feed_dict = {X:train_feature_resize[start:end], keep_prob:0.5, Y:train_emotion[start:end]})\n",
        "                start += batch_size\n",
        "                end += batch_size\n",
        "                if step%100 == 0:\n",
        "                    #print(tf.argmax(softmax, 1).eval())\n",
        "                    print('round: ', i,'step: ' , step)\n",
        "                step += 1\n",
        "                \n",
        "            print('goint to start public prediction')\n",
        "            print('the length of test_pub_emotion :' ,len(test_pub_emotion))\n",
        "            start1 = 0\n",
        "            end1 = start1 + batch_size\n",
        "            k = 0\n",
        "            while(end1 < len(test_pub_emotion)):\n",
        "            #while(end1 < 320):\n",
        "                predict = sess.run(predictions, feed_dict = {X:test_pub_feature_resize[start1:end1], keep_prob:1})\n",
        "                #prediction.append(predict.tolist())\n",
        "                accurate = test_pub_emotion[start1:end1]\n",
        "                    \n",
        "                if  end1%512 == 0:\n",
        "                    print(predict)\n",
        "                    #print(accurate)\n",
        "                \n",
        "                for w in range(len(predict)):\n",
        "                    if predict[w] == accurate[w]:\n",
        "                        k += 1     \n",
        "                            \n",
        "                start1 += batch_size\n",
        "                end1 += batch_size \n",
        "                    \n",
        "            accurate_rate = k / len(test_pub_emotion)\n",
        "            if accurate_rate > max_pub:\n",
        "                max_pub = accurate_rate\n",
        "            print('end public prediction')\n",
        "            print('the public accurate is : ', accurate_rate)\n",
        "            print('the max pub_accurate :', max_pub)\n",
        "            \n",
        "            print('goint to start private prediction')\n",
        "            print('the length of test_pri_emotion :' ,len(test_pri_emotion))\n",
        "            start1 = 0\n",
        "            end1 = start1 + batch_size\n",
        "            k = 0\n",
        "            while(end1 < len(test_pri_emotion)):\n",
        "            #while(end1 < 320):\n",
        "                predict = sess.run(predictions, feed_dict = {X:test_pri_feature_resize[start1:end1], keep_prob:1})\n",
        "                #prediction.append(predict.tolist())\n",
        "                accurate = test_pri_emotion[start1:end1]\n",
        "                    \n",
        "                if  end1%512 == 0:\n",
        "                    print(predict)\n",
        "                    #print(accurate)\n",
        "                \n",
        "                for w in range(len(predict)):\n",
        "                    if predict[w] == accurate[w]:\n",
        "                        k += 1     \n",
        "                            \n",
        "                start1 += batch_size\n",
        "                end1 += batch_size \n",
        "                    \n",
        "            accurate_rate = k / len(test_pub_emotion)\n",
        "            if accurate_rate > max_pri:\n",
        "                max_pri = accurate_rate\n",
        "            print('end public prediction')\n",
        "            print('the public accurate is : ', accurate_rate)\n",
        "            print('the max pri_accurate:', max_pri)\n",
        "                  \n",
        "train_vgg()\n",
        "#use_vgg()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0cf1c4d35d5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive//fer2013//traincpy.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#f_train_resize = open(\"drive//fer2013//trainresize.csv\", encoding = 'UTF-8')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#df_train_resize = pd.read_csv(f_train_resize)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m     \"\"\"\n\u001b[1;32m    779\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Fj18u_K-o13m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "bbf4b437-2950-49b0-fa34-669c1853fc14"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gcgltATMo2pQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 指定Google Drive云端硬盘的根目录，名为drive\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}